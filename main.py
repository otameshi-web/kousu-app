from fastapi import FastAPI, Request, Form, UploadFile, File
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from typing import List
import pandas as pd
import os
import io
import base64
import requests
from datetime import datetime
from collections import defaultdict
import re

# === åŸºæœ¬è¨­å®š ===
app = FastAPI()
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")
CSV_PATH = os.path.join("data", "å·¥æ•°ãƒ‡ãƒ¼ã‚¿.csv")

# === ã‚°ãƒ©ãƒ•UIç³»ãƒ«ãƒ¼ãƒˆ ===
@app.get("/", response_class=HTMLResponse)
async def index(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.get("/graph/menu", response_class=HTMLResponse)
async def graph_menu(request: Request):
    return templates.TemplateResponse("graph_menu.html", {"request": request})

@app.get("/graph/all", response_class=HTMLResponse)
async def graph_all_menu(request: Request):
    return templates.TemplateResponse("graph_all_menu.html", {"request": request})
@app.get("/graph/term", response_class=HTMLResponse)
async def graph_term(request: Request):
    try:
        df = pd.read_csv(CSV_PATH, encoding="utf-8")
    except UnicodeDecodeError:
        df = pd.read_csv(CSV_PATH, encoding="cp932")
    df.columns = [col.strip() for col in df.columns]
    df = df.rename(columns={"ä½œæ¥­æ—¥": "æ—¥ä»˜", "ä½œæ¥­å®Ÿæ–½è€…": "ä½œæ¥­è€…", "ä½œæ¥­ç¨®åˆ¥": "ä½œæ¥­ç¨®åˆ¥", "ä½œæ¥­æ™‚é–“": "æ™‚é–“"})
    df["æ—¥ä»˜"] = pd.to_datetime(df["æ—¥ä»˜"], errors="coerce")
    df = df.dropna(subset=["æ—¥ä»˜", "ä½œæ¥­ç¨®åˆ¥"])

    def get_term(date):
        y = date.year
        return f"{y-1}å¹´5æœˆï½{y}å¹´4æœˆ" if date.month < 5 else f"{y}å¹´5æœˆï½{y+1}å¹´4æœˆ"
    df["æœŸ"] = df["æ—¥ä»˜"].apply(get_term)

    term_list = sorted(df["æœŸ"].unique(), reverse=True)
    work_types = sorted([w for w in df["ä½œæ¥­ç¨®åˆ¥"].unique() if w != "å°è¨ˆ"])

    return templates.TemplateResponse("graph_term.html", {
        "request": request,
        "terms": term_list,
        "work_types": work_types
    })
from collections import defaultdict
import os

@app.post("/graph/term/result", response_class=HTMLResponse)
async def graph_term_result(
    request: Request,
    term: str = Form(...),
    work_types: List[str] = Form(...)
):
    # ã‚«ãƒ©ãƒ¼ãƒ‘ãƒ¬ãƒƒãƒˆï¼ˆ10è‰²ï¼‰
    color_list_rgba = [
        "rgba(31, 119, 180, 0.7)", "rgba(255, 127, 14, 0.7)",
        "rgba(44, 160, 44, 0.7)", "rgba(214, 39, 40, 0.7)",
        "rgba(148, 103, 189, 0.7)", "rgba(140, 86, 75, 0.7)",
        "rgba(227, 119, 194, 0.7)", "rgba(127, 127, 127, 0.7)",
        "rgba(188, 189, 34, 0.7)", "rgba(23, 190, 207, 0.7)"
    ]

    # â–¼ é€šå¸¸ä½œæ¥­ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    try:
        df = pd.read_csv(CSV_PATH, encoding="utf-8")
    except UnicodeDecodeError:
        df = pd.read_csv(CSV_PATH, encoding="cp932")

    df.columns = [col.strip() for col in df.columns]
    df = df.rename(columns={
        "ä½œæ¥­æ—¥": "æ—¥ä»˜",
        "ä½œæ¥­å®Ÿæ–½è€…": "ä½œæ¥­è€…",
        "ä½œæ¥­ç¨®åˆ¥": "ä½œæ¥­ç¨®åˆ¥",
        "ä½œæ¥­æ™‚é–“": "æ™‚é–“"
    })
    df["æ—¥ä»˜"] = pd.to_datetime(df["æ—¥ä»˜"], errors="coerce")
    df = df.dropna(subset=["æ—¥ä»˜", "ä½œæ¥­ç¨®åˆ¥", "ä½œæ¥­è€…", "æ™‚é–“"])
    df["æ™‚é–“"] = pd.to_numeric(df["æ™‚é–“"], errors="coerce")
    df = df.dropna()

    def get_term(date):
        y = date.year
        return f"{y-1}å¹´5æœˆï½{y}å¹´4æœˆ" if date.month < 5 else f"{y}å¹´5æœˆï½{y+1}å¹´4æœˆ"

    df["æœŸ"] = df["æ—¥ä»˜"].apply(get_term)
    df = df[df["æœŸ"] == term]
    df = df[df["ä½œæ¥­ç¨®åˆ¥"].isin(work_types)]
    df = df[~df["ä½œæ¥­è€…"].str.contains("å‰Šé™¤æ¸ˆã¿", na=False)]

    df["å¹´æœˆ"] = df["æ—¥ä»˜"].dt.strftime("%Y-%m")
    grouped = df.groupby(["å¹´æœˆ", "ä½œæ¥­ç¨®åˆ¥"])

    result = defaultdict(lambda: {"æ™‚é–“åˆè¨ˆ": 0.0, "ä»¶æ•°": 0})
    for (ym, wt), group in grouped:
        result[(ym, wt)]["æ™‚é–“åˆè¨ˆ"] += group["æ™‚é–“"].sum()
        result[(ym, wt)]["ä»¶æ•°"] += len(group)

    labels = sorted(set(ym for ym, _ in result))
    time_datasets = []
    count_datasets = []

    for idx, wt in enumerate(work_types):
        color = color_list_rgba[idx % len(color_list_rgba)]
        time_data = [result[(ym, wt)]["æ™‚é–“åˆè¨ˆ"] if (ym, wt) in result else 0 for ym in labels]
        count_data = [result[(ym, wt)]["ä»¶æ•°"] if (ym, wt) in result else 0 for ym in labels]
        time_datasets.append({"label": f"{wt}ï¼ˆæ™‚é–“ï¼‰", "data": time_data, "stack": "main", "backgroundColor": color})
        count_datasets.append({"label": f"{wt}ï¼ˆä»¶æ•°ï¼‰", "data": count_data, "stack": "main", "backgroundColor": color})

    # â–¼ æ¤œæŸ»å·¥æ•°ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆç‚¹æ¤œåŠã³æ¤œæŸ»ã®ã¿ï¼‰
    kensa_totals = {}
    if work_types == ["ç‚¹æ¤œåŠã³æ¤œæŸ»"]:
        kensa_path = os.path.join("data", "æ¤œæŸ»å·¥æ•°ãƒ‡ãƒ¼ã‚¿.csv")
        try:
            kensa_df = pd.read_csv(kensa_path, encoding="utf-8")
        except UnicodeDecodeError:
            kensa_df = pd.read_csv(kensa_path, encoding="cp932")

        kensa_df.columns = [col.strip() for col in kensa_df.columns]
        kensa_df = kensa_df.rename(columns={
            "ä½œæ¥­æ—¥": "æ—¥ä»˜",
            "ä½œæ¥­ID": "ä½œæ¥­ID",
            "ä½œæ¥­å®Ÿæ–½è€…": "ä½œæ¥­è€…",
            "ä½œæ¥­é …ç›®ï¼ˆç®‡æ‰€ï¼‰": "é …ç›®",
            "ä½œæ¥­æ™‚é–“": "æ™‚é–“"
        })
        kensa_df["æ—¥ä»˜"] = pd.to_datetime(kensa_df["æ—¥ä»˜"], errors="coerce")
        kensa_df["æ™‚é–“"] = pd.to_numeric(kensa_df["æ™‚é–“"], errors="coerce")
        kensa_df = kensa_df.dropna(subset=["æ—¥ä»˜", "é …ç›®", "ä½œæ¥­ID"])
        kensa_df["å¹´æœˆ"] = kensa_df["æ—¥ä»˜"].dt.strftime("%Y-%m")
        kensa_df = kensa_df[kensa_df["é …ç›®"].isin(["æ³•å®šæ¤œæŸ»", "ç¤¾å†…æ¤œæŸ»"])]
        kensa_df = kensa_df.drop_duplicates(subset=["ä½œæ¥­ID", "é …ç›®"])

        grouped_kensa = kensa_df.groupby(["å¹´æœˆ", "é …ç›®"]).agg(
            ä»¶æ•°=("ä½œæ¥­ID", "count"),
            åˆè¨ˆæ™‚é–“=("æ™‚é–“", "sum")
        ).reset_index()

        result_kensa = defaultdict(lambda: {"æ³•å®šæ¤œæŸ»_ä»¶æ•°": 0, "æ³•å®šæ¤œæŸ»_æ™‚é–“": 0, "ç¤¾å†…æ¤œæŸ»_ä»¶æ•°": 0, "ç¤¾å†…æ¤œæŸ»_æ™‚é–“": 0})
        for _, row in grouped_kensa.iterrows():
            ym = row["å¹´æœˆ"]
            if row["é …ç›®"] == "æ³•å®šæ¤œæŸ»":
                result_kensa[ym]["æ³•å®šæ¤œæŸ»_ä»¶æ•°"] = row["ä»¶æ•°"]
                result_kensa[ym]["æ³•å®šæ¤œæŸ»_æ™‚é–“"] = row["åˆè¨ˆæ™‚é–“"]
            elif row["é …ç›®"] == "ç¤¾å†…æ¤œæŸ»":
                result_kensa[ym]["ç¤¾å†…æ¤œæŸ»_ä»¶æ•°"] = row["ä»¶æ•°"]
                result_kensa[ym]["ç¤¾å†…æ¤œæŸ»_æ™‚é–“"] = row["åˆè¨ˆæ™‚é–“"]

        for ym in labels:
            total_time = result_kensa[ym]["æ³•å®šæ¤œæŸ»_æ™‚é–“"] + result_kensa[ym]["ç¤¾å†…æ¤œæŸ»_æ™‚é–“"]
            total_count = result_kensa[ym]["æ³•å®šæ¤œæŸ»_ä»¶æ•°"] + result_kensa[ym]["ç¤¾å†…æ¤œæŸ»_ä»¶æ•°"]
            kensa_totals[ym] = {"æ™‚é–“åˆè¨ˆ": total_time, "ä»¶æ•°åˆè¨ˆ": total_count}

        # æ¤œæŸ»ãƒ‡ãƒ¼ã‚¿ã¯2è‰²å›ºå®š
        time_datasets.append({
            "label": "æ³•å®šæ¤œæŸ»",
            "data": [result_kensa[ym]["æ³•å®šæ¤œæŸ»_æ™‚é–“"] if ym in result_kensa else 0 for ym in labels],
            "stack": "æ¤œæŸ»",
            "backgroundColor": "rgba(255, 127, 14, 0.7)"
        })
        time_datasets.append({
            "label": "ç¤¾å†…æ¤œæŸ»",
            "data": [result_kensa[ym]["ç¤¾å†…æ¤œæŸ»_æ™‚é–“"] if ym in result_kensa else 0 for ym in labels],
            "stack": "æ¤œæŸ»",
            "backgroundColor": "rgba(44, 160, 44, 0.7)"
        })
        count_datasets.append({
            "label": "æ³•å®šæ¤œæŸ»",
            "data": [result_kensa[ym]["æ³•å®šæ¤œæŸ»_ä»¶æ•°"] if ym in result_kensa else 0 for ym in labels],
            "stack": "æ¤œæŸ»",
            "backgroundColor": "rgba(255, 127, 14, 0.7)"
        })
        count_datasets.append({
            "label": "ç¤¾å†…æ¤œæŸ»",
            "data": [result_kensa[ym]["ç¤¾å†…æ¤œæŸ»_ä»¶æ•°"] if ym in result_kensa else 0 for ym in labels],
            "stack": "æ¤œæŸ»",
            "backgroundColor": "rgba(44, 160, 44, 0.7)"
        })

    return templates.TemplateResponse("graph_term_result.html", {
        "request": request,
        "term": term,
        "labels": labels,
        "time_datasets": time_datasets,
        "count_datasets": count_datasets,
        "work_types": work_types,
        "kensa_totals": kensa_totals
    })


# ==========================
# Part 3: æœˆåˆ¥ãƒ»å€‹äººæ¯”è¼ƒé–¢é€£
# ==========================

@app.get("/graph/month", response_class=HTMLResponse)
async def graph_month(request: Request):
    try:
        df = pd.read_csv(CSV_PATH, encoding="utf-8")
    except UnicodeDecodeError:
        df = pd.read_csv(CSV_PATH, encoding="cp932")
    df.columns = [col.strip() for col in df.columns]
    df = df.rename(columns={"ä½œæ¥­æ—¥": "æ—¥ä»˜", "ä½œæ¥­å®Ÿæ–½è€…": "ä½œæ¥­è€…", "ä½œæ¥­ç¨®åˆ¥": "ä½œæ¥­ç¨®åˆ¥", "ä½œæ¥­æ™‚é–“": "æ™‚é–“"})
    df["æ—¥ä»˜"] = pd.to_datetime(df["æ—¥ä»˜"], errors="coerce")
    df = df.dropna(subset=["æ—¥ä»˜", "ä½œæ¥­ç¨®åˆ¥"])

    df["å¹´"] = df["æ—¥ä»˜"].dt.year
    df["æœˆ"] = df["æ—¥ä»˜"].dt.month
    work_types = sorted([w for w in df["ä½œæ¥­ç¨®åˆ¥"].unique() if w != "å°è¨ˆ"])
    years = sorted(df["å¹´"].unique(), reverse=True)
    months = sorted(df["æœˆ"].unique())

    return templates.TemplateResponse("graph_month.html", {
        "request": request,
        "years": years,
        "months": months,
        "work_types": work_types
    })

from collections import defaultdict
import os

@app.post("/graph/month/result", response_class=HTMLResponse)
async def graph_month_result(
    request: Request,
    year: int = Form(...),
    month: int = Form(...),
    work_types: List[str] = Form(...)
):
    # â–¼ ã‚«ãƒ©ãƒ¼ãƒ‘ãƒ¬ãƒƒãƒˆ
    color_list_rgba = [
        "rgba(31, 119, 180, 0.7)", "rgba(255, 127, 14, 0.7)",
        "rgba(44, 160, 44, 0.7)", "rgba(214, 39, 40, 0.7)",
        "rgba(148, 103, 189, 0.7)", "rgba(140, 86, 75, 0.7)",
        "rgba(227, 119, 194, 0.7)", "rgba(127, 127, 127, 0.7)",
        "rgba(188, 189, 34, 0.7)", "rgba(23, 190, 207, 0.7)"
    ]

    # â–¼ CSVèª­ã¿è¾¼ã¿ï¼ˆé€šå¸¸ä½œæ¥­ãƒ‡ãƒ¼ã‚¿ï¼‰
    try:
        df = pd.read_csv(CSV_PATH, encoding="utf-8")
    except UnicodeDecodeError:
        df = pd.read_csv(CSV_PATH, encoding="cp932")

    df.columns = [col.strip() for col in df.columns]
    df = df.rename(columns={"ä½œæ¥­æ—¥": "æ—¥ä»˜", "ä½œæ¥­å®Ÿæ–½è€…": "ä½œæ¥­è€…", "ä½œæ¥­ç¨®åˆ¥": "ä½œæ¥­ç¨®åˆ¥", "ä½œæ¥­æ™‚é–“": "æ™‚é–“"})
    df["æ—¥ä»˜"] = pd.to_datetime(df["æ—¥ä»˜"], errors="coerce")
    df["æ™‚é–“"] = pd.to_numeric(df["æ™‚é–“"], errors="coerce")
    df = df.dropna(subset=["æ—¥ä»˜", "ä½œæ¥­è€…", "ä½œæ¥­ç¨®åˆ¥", "æ™‚é–“"])

    df = df[(df["æ—¥ä»˜"].dt.year == year) & (df["æ—¥ä»˜"].dt.month == month)]
    df = df[df["ä½œæ¥­ç¨®åˆ¥"].isin(work_types)]
    df = df[~df["ä½œæ¥­è€…"].str.contains("å‰Šé™¤æ¸ˆã¿", na=False)]

    # â–¼ é€šå¸¸ä½œæ¥­é›†è¨ˆ
    result = defaultdict(lambda: {"æ™‚é–“åˆè¨ˆ": 0.0, "ä»¶æ•°": 0})
    for _, row in df.iterrows():
        key = (row["ä½œæ¥­è€…"], row["ä½œæ¥­ç¨®åˆ¥"])
        result[key]["æ™‚é–“åˆè¨ˆ"] += row["æ™‚é–“"]
        result[key]["ä»¶æ•°"] += 1

    users = sorted(set(k[0] for k in result))
    time_datasets = []
    count_datasets = []

    for idx, wt in enumerate(work_types):
        color = color_list_rgba[idx % len(color_list_rgba)]
        time_data = [result[(u, wt)]["æ™‚é–“åˆè¨ˆ"] if (u, wt) in result else 0 for u in users]
        count_data = [result[(u, wt)]["ä»¶æ•°"] if (u, wt) in result else 0 for u in users]
        time_datasets.append({"label": f"{wt}ï¼ˆæ™‚é–“ï¼‰", "data": time_data, "stack": "main", "backgroundColor": color})
        count_datasets.append({"label": f"{wt}ï¼ˆä»¶æ•°ï¼‰", "data": count_data, "stack": "main", "backgroundColor": color})

    # â–¼ ã€ç‚¹æ¤œåŠã³æ¤œæŸ»ã€‘ã®ã¿é¸æŠæ™‚ã®æ¤œæŸ»ãƒ‡ãƒ¼ã‚¿é›†è¨ˆ
    kensa_totals = {}
    if work_types == ["ç‚¹æ¤œåŠã³æ¤œæŸ»"]:
        kensa_path = os.path.join("data", "æ¤œæŸ»å·¥æ•°ãƒ‡ãƒ¼ã‚¿.csv")
        try:
            kensa_df = pd.read_csv(kensa_path, encoding="utf-8")
        except UnicodeDecodeError:
            kensa_df = pd.read_csv(kensa_path, encoding="cp932")

        kensa_df.columns = [col.strip() for col in kensa_df.columns]
        kensa_df = kensa_df.rename(columns={
            "ä½œæ¥­æ—¥": "æ—¥ä»˜",
            "ä½œæ¥­ID": "ä½œæ¥­ID",
            "ä½œæ¥­å®Ÿæ–½è€…": "ä½œæ¥­è€…",
            "ä½œæ¥­é …ç›®ï¼ˆç®‡æ‰€ï¼‰": "é …ç›®",
            "ä½œæ¥­æ™‚é–“": "æ™‚é–“"
        })
        kensa_df["æ—¥ä»˜"] = pd.to_datetime(kensa_df["æ—¥ä»˜"], errors="coerce")
        kensa_df["æ™‚é–“"] = pd.to_numeric(kensa_df["æ™‚é–“"], errors="coerce")
        kensa_df = kensa_df.dropna(subset=["æ—¥ä»˜", "ä½œæ¥­è€…", "é …ç›®", "æ™‚é–“", "ä½œæ¥­ID"])
        kensa_df = kensa_df[(kensa_df["æ—¥ä»˜"].dt.year == year) & (kensa_df["æ—¥ä»˜"].dt.month == month)]
        kensa_df = kensa_df[kensa_df["é …ç›®"].isin(["æ³•å®šæ¤œæŸ»", "ç¤¾å†…æ¤œæŸ»"])]
        kensa_df = kensa_df.drop_duplicates(subset=["ä½œæ¥­ID", "é …ç›®"])

        grouped_kensa = kensa_df.groupby(["ä½œæ¥­è€…", "é …ç›®"]).agg(
            ä»¶æ•°=("ä½œæ¥­ID", "count"),
            åˆè¨ˆæ™‚é–“=("æ™‚é–“", "sum")
        ).reset_index()

        kensa_data = defaultdict(lambda: {"æ³•å®šæ¤œæŸ»_æ™‚é–“": 0, "ç¤¾å†…æ¤œæŸ»_æ™‚é–“": 0, "æ³•å®šæ¤œæŸ»_ä»¶æ•°": 0, "ç¤¾å†…æ¤œæŸ»_ä»¶æ•°": 0})
        for _, row in grouped_kensa.iterrows():
            user = row["ä½œæ¥­è€…"]
            if row["é …ç›®"] == "æ³•å®šæ¤œæŸ»":
                kensa_data[user]["æ³•å®šæ¤œæŸ»_æ™‚é–“"] = row["åˆè¨ˆæ™‚é–“"]
                kensa_data[user]["æ³•å®šæ¤œæŸ»_ä»¶æ•°"] = row["ä»¶æ•°"]
            elif row["é …ç›®"] == "ç¤¾å†…æ¤œæŸ»":
                kensa_data[user]["ç¤¾å†…æ¤œæŸ»_æ™‚é–“"] = row["åˆè¨ˆæ™‚é–“"]
                kensa_data[user]["ç¤¾å†…æ¤œæŸ»_ä»¶æ•°"] = row["ä»¶æ•°"]

        for u in users:
            total_time = kensa_data[u]["æ³•å®šæ¤œæŸ»_æ™‚é–“"] + kensa_data[u]["ç¤¾å†…æ¤œæŸ»_æ™‚é–“"]
            total_count = kensa_data[u]["æ³•å®šæ¤œæŸ»_ä»¶æ•°"] + kensa_data[u]["ç¤¾å†…æ¤œæŸ»_ä»¶æ•°"]
            kensa_totals[u] = {"æ™‚é–“åˆè¨ˆ": total_time, "ä»¶æ•°åˆè¨ˆ": total_count}

        # æ¤œæŸ»ãƒ‡ãƒ¼ã‚¿ã¯2è‰²å›ºå®š
        time_datasets.append({
            "label": "æ³•å®šæ¤œæŸ»",
            "data": [kensa_data[u]["æ³•å®šæ¤œæŸ»_æ™‚é–“"] if u in kensa_data else 0 for u in users],
            "stack": "æ¤œæŸ»",
            "backgroundColor": "rgba(255, 127, 14, 0.7)"
        })
        time_datasets.append({
            "label": "ç¤¾å†…æ¤œæŸ»",
            "data": [kensa_data[u]["ç¤¾å†…æ¤œæŸ»_æ™‚é–“"] if u in kensa_data else 0 for u in users],
            "stack": "æ¤œæŸ»",
            "backgroundColor": "rgba(44, 160, 44, 0.7)"
        })
        count_datasets.append({
            "label": "æ³•å®šæ¤œæŸ»",
            "data": [kensa_data[u]["æ³•å®šæ¤œæŸ»_ä»¶æ•°"] if u in kensa_data else 0 for u in users],
            "stack": "æ¤œæŸ»",
            "backgroundColor": "rgba(255, 127, 14, 0.7)"
        })
        count_datasets.append({
            "label": "ç¤¾å†…æ¤œæŸ»",
            "data": [kensa_data[u]["ç¤¾å†…æ¤œæŸ»_ä»¶æ•°"] if u in kensa_data else 0 for u in users],
            "stack": "æ¤œæŸ»",
            "backgroundColor": "rgba(44, 160, 44, 0.7)"
        })

    return templates.TemplateResponse("graph_month_result.html", {
        "request": request,
        "year": year,
        "month": month,
        "labels": users,
        "time_datasets": time_datasets,
        "count_datasets": count_datasets,
        "work_types": work_types,
        "kensa_totals": kensa_totals
    })


# ==========================
# Part 4: å€‹äººã‚°ãƒ©ãƒ•å‡¦ç†
# ==========================

@app.get("/graph/person", response_class=HTMLResponse)
async def graph_person_menu(request: Request):
    return templates.TemplateResponse("graph_person_menu.html", {"request": request})

@app.get("/graph/person/type", response_class=HTMLResponse)
async def graph_person_type_input(request: Request):
    try:
        try:
            df = pd.read_csv(CSV_PATH, encoding="utf-8")
        except UnicodeDecodeError:
            df = pd.read_csv(CSV_PATH, encoding="cp932")

        df.columns = [col.strip() for col in df.columns]
        df = df.rename(columns={
            "ä½œæ¥­æ—¥": "æ—¥ä»˜",
            "ä½œæ¥­å®Ÿæ–½è€…": "ä½œæ¥­è€…",
            "ä½œæ¥­ç¨®åˆ¥": "ä½œæ¥­ç¨®åˆ¥",
            "ä½œæ¥­æ™‚é–“": "æ™‚é–“"
        })

        df["æ—¥ä»˜"] = pd.to_datetime(df["æ—¥ä»˜"], errors="coerce")
        df = df.dropna(subset=["æ—¥ä»˜", "ä½œæ¥­è€…"])

        df["å¹´"] = df["æ—¥ä»˜"].dt.year
        df["æœˆ"] = df["æ—¥ä»˜"].dt.month

        years = sorted(df["å¹´"].unique(), reverse=True)
        months = list(range(1, 13))
        users = sorted(df["ä½œæ¥­è€…"].dropna().unique())
        users = [u for u in users if "å‰Šé™¤æ¸ˆã¿" not in u]

        return templates.TemplateResponse("graph_person_type.html", {
            "request": request,
            "years": years,
            "months": months,
            "users": users
        })

    except Exception as e:
        return HTMLResponse(f"ã‚¨ãƒ©ãƒ¼: {e}", status_code=500)


# ä½œæ¥­ç¨®åˆ¥æ¯”è¼ƒè¡¨ï¼ˆè¡¨ç¤ºï¼‰
# ä½œæ¥­ç¨®åˆ¥æ¯”è¼ƒè¡¨ï¼ˆè¡¨ç¤ºï¼‰
@app.post("/graph/person/type/result", response_class=HTMLResponse)
async def graph_person_type_result(
    request: Request,
    year: int = Form(...),
    month: int = Form(...),
    user: str = Form(...)
):
    try:
        df = pd.read_csv(CSV_PATH, encoding="utf-8")
    except UnicodeDecodeError:
        df = pd.read_csv(CSV_PATH, encoding="cp932")

    df.columns = [col.strip() for col in df.columns]
    df = df.rename(columns={
        "ä½œæ¥­æ—¥": "æ—¥ä»˜", "ä½œæ¥­å®Ÿæ–½è€…": "ä½œæ¥­è€…",
        "ä½œæ¥­ç¨®åˆ¥": "ä½œæ¥­ç¨®åˆ¥", "ä½œæ¥­æ™‚é–“": "æ™‚é–“"
    })
    df["æ—¥ä»˜"] = pd.to_datetime(df["æ—¥ä»˜"], errors="coerce")
    df["æ™‚é–“"] = pd.to_numeric(df["æ™‚é–“"], errors="coerce")
    df = df.dropna(subset=["æ—¥ä»˜", "ä½œæ¥­è€…", "ä½œæ¥­ç¨®åˆ¥", "æ™‚é–“"])

    df = df[
        (df["æ—¥ä»˜"].dt.year == year) &
        (df["æ—¥ä»˜"].dt.month == month) &
        (df["ä½œæ¥­è€…"] == user)
    ]
    df = df[df["ä½œæ¥­ç¨®åˆ¥"] != "å°è¨ˆ"]

    grouped = df.groupby("ä½œæ¥­ç¨®åˆ¥")
    result = {
        wt: {
            "æ™‚é–“åˆè¨ˆ": float(group["æ™‚é–“"].sum()),
            "ä»¶æ•°": int(len(group))
        }
        for wt, group in grouped
    }

    # åˆè¨ˆãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
    total_time = sum(item["æ™‚é–“åˆè¨ˆ"] for item in result.values())
    total_count = sum(item["ä»¶æ•°"] for item in result.values())
    result["åˆè¨ˆ"] = {
        "æ™‚é–“åˆè¨ˆ": total_time,
        "ä»¶æ•°": total_count
    }

    labels = list(result.keys())
    time_data = [result[wt]["æ™‚é–“åˆè¨ˆ"] for wt in labels]
    count_data = [result[wt]["ä»¶æ•°"] for wt in labels]

    datasets = [
        {"label": "æ™‚é–“ï¼ˆhï¼‰", "data": time_data},
        {"label": "ä»¶æ•°ï¼ˆä»¶ï¼‰", "data": count_data}
    ]

    return templates.TemplateResponse("graph_person_type_result.html", {
        "request": request,
        "labels": labels,
        "datasets": datasets,
        "year": year,
        "month": month,
        "user": user
    })

@app.get("/graph/person/period", response_class=HTMLResponse)
async def graph_person_period_input(request: Request):
    try:
        try:
            df = pd.read_csv(CSV_PATH, encoding="utf-8")
        except UnicodeDecodeError:
            df = pd.read_csv(CSV_PATH, encoding="cp932")

        df.columns = [col.strip() for col in df.columns]
        df = df.rename(columns={
            "ä½œæ¥­æ—¥": "æ—¥ä»˜",
            "ä½œæ¥­å®Ÿæ–½è€…": "ä½œæ¥­è€…",
            "ä½œæ¥­ç¨®åˆ¥": "ä½œæ¥­ç¨®åˆ¥",
            "ä½œæ¥­æ™‚é–“": "æ™‚é–“"
        })

        df["æ—¥ä»˜"] = pd.to_datetime(df["æ—¥ä»˜"], errors="coerce")
        df = df.dropna(subset=["æ—¥ä»˜", "ä½œæ¥­è€…", "ä½œæ¥­ç¨®åˆ¥"])

        df["å¹´"] = df["æ—¥ä»˜"].dt.year
        df["æœˆ"] = df["æ—¥ä»˜"].dt.month

        years = sorted(df["å¹´"].unique(), reverse=True)
        months = list(range(1, 13))
        users = sorted(df["ä½œæ¥­è€…"].dropna().unique())
        users = [u for u in users if "å‰Šé™¤æ¸ˆã¿" not in u]

        work_types = sorted(df["ä½œæ¥­ç¨®åˆ¥"].unique())
        work_types = [w for w in work_types if w != "å°è¨ˆ"]

        return templates.TemplateResponse("graph_person_period.html", {
            "request": request,
            "years": years,
            "months": months,
            "users": users,
            "work_types": work_types
        })

    except Exception as e:
        return HTMLResponse(f"ã‚¨ãƒ©ãƒ¼ï¼š{e}", status_code=500)

# æœŸé–“æŒ‡å®šæ¯”è¼ƒè¡¨ï¼ˆè¡¨ç¤ºï¼‰
from collections import defaultdict
import os
from pandas.tseries.offsets import MonthEnd  # è¿½åŠ 

@app.post("/graph/person/period/result", response_class=HTMLResponse)
async def graph_person_period_result(
    request: Request,
    start_year: int = Form(...),
    start_month: int = Form(...),
    end_year: int = Form(...),
    end_month: int = Form(...),
    user: str = Form(...),
    work_types: List[str] = Form(...)
):
    # â–¼ ã‚«ãƒ©ãƒ¼ãƒ‘ãƒ¬ãƒƒãƒˆ
    color_list_rgba = [
        "rgba(31, 119, 180, 0.7)", "rgba(255, 127, 14, 0.7)",
        "rgba(44, 160, 44, 0.7)", "rgba(214, 39, 40, 0.7)",
        "rgba(148, 103, 189, 0.7)", "rgba(140, 86, 75, 0.7)",
        "rgba(227, 119, 194, 0.7)", "rgba(127, 127, 127, 0.7)",
        "rgba(188, 189, 34, 0.7)", "rgba(23, 190, 207, 0.7)"
    ]

    # â–¼ é€šå¸¸ä½œæ¥­ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    try:
        df = pd.read_csv(CSV_PATH, encoding="utf-8")
    except UnicodeDecodeError:
        df = pd.read_csv(CSV_PATH, encoding="cp932")

    df.columns = [col.strip() for col in df.columns]
    df = df.rename(columns={
        "ä½œæ¥­æ—¥": "æ—¥ä»˜", "ä½œæ¥­å®Ÿæ–½è€…": "ä½œæ¥­è€…",
        "ä½œæ¥­ç¨®åˆ¥": "ä½œæ¥­ç¨®åˆ¥", "ä½œæ¥­æ™‚é–“": "æ™‚é–“"
    })
    df["æ—¥ä»˜"] = pd.to_datetime(df["æ—¥ä»˜"], errors="coerce")
    df["æ™‚é–“"] = pd.to_numeric(df["æ™‚é–“"], errors="coerce")
    df = df.dropna(subset=["æ—¥ä»˜", "ä½œæ¥­è€…", "ä½œæ¥­ç¨®åˆ¥", "æ™‚é–“"])

    df = df[df["ä½œæ¥­è€…"] == user]
    df = df[df["ä½œæ¥­ç¨®åˆ¥"].isin(work_types)]
    df = df[df["ä½œæ¥­ç¨®åˆ¥"] != "å°è¨ˆ"]

    df["å¹´æœˆ"] = df["æ—¥ä»˜"].dt.strftime("%Y-%m")
    start = pd.to_datetime(f"{start_year}-{start_month:02d}")
    end = pd.to_datetime(f"{end_year}-{end_month:02d}") + MonthEnd(0)  # ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆï¼
    df = df[(df["æ—¥ä»˜"] >= start) & (df["æ—¥ä»˜"] <= end)]

    grouped = df.groupby(["å¹´æœˆ", "ä½œæ¥­ç¨®åˆ¥"])
    result = defaultdict(lambda: {"æ™‚é–“åˆè¨ˆ": 0.0, "ä»¶æ•°": 0})
    for (ym, wt), group in grouped:
        result[(ym, wt)]["æ™‚é–“åˆè¨ˆ"] += group["æ™‚é–“"].sum()
        result[(ym, wt)]["ä»¶æ•°"] += len(group)

    ym_labels = sorted(set(ym for ym, _ in result))
    time_datasets = []
    count_datasets = []

    for idx, wt in enumerate(work_types):
        color = color_list_rgba[idx % len(color_list_rgba)]
        time_data = [result[(ym, wt)]["æ™‚é–“åˆè¨ˆ"] if (ym, wt) in result else 0 for ym in ym_labels]
        count_data = [result[(ym, wt)]["ä»¶æ•°"] if (ym, wt) in result else 0 for ym in ym_labels]
        time_datasets.append({"label": f"{wt}ï¼ˆæ™‚é–“ï¼‰", "data": time_data, "stack": "main", "backgroundColor": color})
        count_datasets.append({"label": f"{wt}ï¼ˆä»¶æ•°ï¼‰", "data": count_data, "stack": "main", "backgroundColor": color})

    # â–¼ æ¤œæŸ»å·¥æ•°ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆç‚¹æ¤œåŠã³æ¤œæŸ»ã®ã¿ï¼‰
    kensa_totals = {}
    if work_types == ["ç‚¹æ¤œåŠã³æ¤œæŸ»"]:
        kensa_path = os.path.join("data", "æ¤œæŸ»å·¥æ•°ãƒ‡ãƒ¼ã‚¿.csv")
        try:
            kensa_df = pd.read_csv(kensa_path, encoding="utf-8")
        except UnicodeDecodeError:
            kensa_df = pd.read_csv(kensa_path, encoding="cp932")

        kensa_df.columns = [col.strip() for col in kensa_df.columns]
        kensa_df = kensa_df.rename(columns={
            "ä½œæ¥­æ—¥": "æ—¥ä»˜",
            "ä½œæ¥­ID": "ä½œæ¥­ID",
            "ä½œæ¥­å®Ÿæ–½è€…": "ä½œæ¥­è€…",
            "ä½œæ¥­é …ç›®ï¼ˆç®‡æ‰€ï¼‰": "é …ç›®",
            "ä½œæ¥­æ™‚é–“": "æ™‚é–“"
        })
        kensa_df["æ—¥ä»˜"] = pd.to_datetime(kensa_df["æ—¥ä»˜"], errors="coerce")
        kensa_df["æ™‚é–“"] = pd.to_numeric(kensa_df["æ™‚é–“"], errors="coerce")
        kensa_df = kensa_df.dropna(subset=["æ—¥ä»˜", "é …ç›®", "ä½œæ¥­ID"])
        kensa_df["å¹´æœˆ"] = kensa_df["æ—¥ä»˜"].dt.strftime("%Y-%m")
        kensa_df = kensa_df[kensa_df["é …ç›®"].isin(["æ³•å®šæ¤œæŸ»", "ç¤¾å†…æ¤œæŸ»"])]
        kensa_df = kensa_df[kensa_df["ä½œæ¥­è€…"] == user]
        kensa_df = kensa_df[(kensa_df["æ—¥ä»˜"] >= start) & (kensa_df["æ—¥ä»˜"] <= end)]
        kensa_df = kensa_df.drop_duplicates(subset=["ä½œæ¥­ID", "é …ç›®"])

        grouped_kensa = kensa_df.groupby(["å¹´æœˆ", "é …ç›®"]).agg(
            ä»¶æ•°=("ä½œæ¥­ID", "count"),
            åˆè¨ˆæ™‚é–“=("æ™‚é–“", "sum")
        ).reset_index()

        result_kensa = defaultdict(lambda: {"æ³•å®šæ¤œæŸ»_ä»¶æ•°": 0, "æ³•å®šæ¤œæŸ»_æ™‚é–“": 0, "ç¤¾å†…æ¤œæŸ»_ä»¶æ•°": 0, "ç¤¾å†…æ¤œæŸ»_æ™‚é–“": 0})
        for _, row in grouped_kensa.iterrows():
            ym = row["å¹´æœˆ"]
            if row["é …ç›®"] == "æ³•å®šæ¤œæŸ»":
                result_kensa[ym]["æ³•å®šæ¤œæŸ»_ä»¶æ•°"] = row["ä»¶æ•°"]
                result_kensa[ym]["æ³•å®šæ¤œæŸ»_æ™‚é–“"] = row["åˆè¨ˆæ™‚é–“"]
            elif row["é …ç›®"] == "ç¤¾å†…æ¤œæŸ»":
                result_kensa[ym]["ç¤¾å†…æ¤œæŸ»_ä»¶æ•°"] = row["ä»¶æ•°"]
                result_kensa[ym]["ç¤¾å†…æ¤œæŸ»_æ™‚é–“"] = row["åˆè¨ˆæ™‚é–“"]

        for ym in ym_labels:
            total_time = result_kensa[ym]["æ³•å®šæ¤œæŸ»_æ™‚é–“"] + result_kensa[ym]["ç¤¾å†…æ¤œæŸ»_æ™‚é–“"]
            total_count = result_kensa[ym]["æ³•å®šæ¤œæŸ»_ä»¶æ•°"] + result_kensa[ym]["ç¤¾å†…æ¤œæŸ»_ä»¶æ•°"]
            kensa_totals[ym] = {"æ™‚é–“åˆè¨ˆ": total_time, "ä»¶æ•°åˆè¨ˆ": total_count}

        # æ¤œæŸ»ãƒ‡ãƒ¼ã‚¿ï¼ˆ2è‰²å›ºå®šï¼‰
        time_datasets.append({
            "label": "æ³•å®šæ¤œæŸ»",
            "data": [result_kensa[ym]["æ³•å®šæ¤œæŸ»_æ™‚é–“"] if ym in result_kensa else 0 for ym in ym_labels],
            "stack": "æ¤œæŸ»",
            "backgroundColor": "rgba(255, 127, 14, 0.7)"
        })
        time_datasets.append({
            "label": "ç¤¾å†…æ¤œæŸ»",
            "data": [result_kensa[ym]["ç¤¾å†…æ¤œæŸ»_æ™‚é–“"] if ym in result_kensa else 0 for ym in ym_labels],
            "stack": "æ¤œæŸ»",
            "backgroundColor": "rgba(44, 160, 44, 0.7)"
        })
        count_datasets.append({
            "label": "æ³•å®šæ¤œæŸ»",
            "data": [result_kensa[ym]["æ³•å®šæ¤œæŸ»_ä»¶æ•°"] if ym in result_kensa else 0 for ym in ym_labels],
            "stack": "æ¤œæŸ»",
            "backgroundColor": "rgba(255, 127, 14, 0.7)"
        })
        count_datasets.append({
            "label": "ç¤¾å†…æ¤œæŸ»",
            "data": [result_kensa[ym]["ç¤¾å†…æ¤œæŸ»_ä»¶æ•°"] if ym in result_kensa else 0 for ym in ym_labels],
            "stack": "æ¤œæŸ»",
            "backgroundColor": "rgba(44, 160, 44, 0.7)"
        })

    return templates.TemplateResponse("graph_person_period_result.html", {
        "request": request,
        "labels": ym_labels,
        "time_datasets": time_datasets,
        "count_datasets": count_datasets,
        "user": user,
        "start": f"{start_year}å¹´{start_month}æœˆ",
        "end": f"{end_year}å¹´{end_month}æœˆ",
        "work_types": work_types,
        "kensa_totals": kensa_totals
    })

# ==========================
#       APIé€£æº
# ==========================
@app.post("/api/receive_data")
async def receive_data(records: UploadFile = File(...)):
    contents = await records.read()

    # CSVèª­ã¿è¾¼ã¿
    try:
        new_df = pd.read_csv(io.BytesIO(contents), encoding="utf-8-sig")
    except UnicodeDecodeError:
        new_df = pd.read_csv(io.BytesIO(contents), encoding="cp932")

    # ã‚«ãƒ©ãƒ åã®æ­£è¦åŒ–
    new_df.columns = [col.strip().replace('"', "").replace("'", "") for col in new_df.columns]

    print("ğŸ” CSVã‚«ãƒ©ãƒ :", new_df.columns.tolist())
    print("ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:\n", new_df.head())

    # ä½œæ¥­æ™‚é–“å‡¦ç†
    time_col = next((col for col in new_df.columns if "ä½œæ¥­æ™‚é–“" in col), None)
    new_df["ä½œæ¥­æ™‚é–“"] = pd.to_numeric(new_df[time_col], errors="coerce") if time_col else 0.0

    # ã‚«ãƒ©ãƒ æƒãˆ
    expected_cols = ["ä½œæ¥­ID", "ä½œæ¥­æ—¥", "ä½œæ¥­å®Ÿæ–½è€…", "ä½œæ¥­é …ç›®ï¼ˆç®‡æ‰€ï¼‰", "ä½œæ¥­æ™‚é–“"]
    new_df = new_df[[col for col in new_df.columns if col in expected_cols]]
    new_df = new_df.reindex(columns=expected_cols)

    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­è¾¼
    os.makedirs("data", exist_ok=True)
    save_path = os.path.join("data", "æ¤œæŸ»å·¥æ•°ãƒ‡ãƒ¼ã‚¿.csv")

    if os.path.exists(save_path) and os.path.getsize(save_path) > 0:
        try:
            existing_df = pd.read_csv(save_path, encoding="utf-8-sig")
        except UnicodeDecodeError:
            existing_df = pd.read_csv(save_path, encoding="cp932")
    else:
        existing_df = pd.DataFrame(columns=expected_cols)

    # ã‚«ãƒ©ãƒ æ•´å½¢
    existing_df.columns = [col.strip() for col in existing_df.columns]
    existing_df = existing_df[[col for col in existing_df.columns if col in expected_cols]]
    existing_df = existing_df.reindex(columns=expected_cols)

    # ğŸ” å·®åˆ†æ›´æ–°å‡¦ç†
    key_cols = ["ä½œæ¥­ID", "ä½œæ¥­é …ç›®ï¼ˆç®‡æ‰€ï¼‰"]
    try:
        existing_df.set_index(key_cols, inplace=True)
        new_df.set_index(key_cols, inplace=True)
    except KeyError:
        return JSONResponse(status_code=400, content={
            "status": "error",
            "message": "ä½œæ¥­IDã¾ãŸã¯ä½œæ¥­é …ç›®ï¼ˆç®‡æ‰€ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚CSVåˆ—åã‚’ã”ç¢ºèªãã ã•ã„ã€‚"
        })

    # è¿½åŠ ï¼‹å¤‰æ›´åæ˜ ï¼ˆè¡Œå·®åˆ†ãŒã‚ã‚‹å ´åˆã‚‚åæ˜ ï¼‰
    updated_df = existing_df.combine_first(new_df)
    updated_df.update(new_df)

    # ä¿å­˜
    updated_df.reset_index(inplace=True)
    updated_df.to_csv(save_path, index=False, encoding="utf-8-sig")

    # GitHubã¸Push
    try:
        GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
        repo_owner = "otameshi-web"
        repo_name = "kousu-app"
        branch = "master"
        file_path = "data/æ¤œæŸ»å·¥æ•°ãƒ‡ãƒ¼ã‚¿.csv"
        api_url = f"https://api.github.com/repos/{repo_owner}/{repo_name}/contents/{file_path}"

        headers = {
            "Authorization": f"Bearer {GITHUB_TOKEN}",
            "Accept": "application/vnd.github+json"
        }
        get_resp = requests.get(api_url, headers=headers, params={"ref": branch})
        sha = get_resp.json().get("sha", None)

        with open(save_path, "rb") as f:
            encoded_content = base64.b64encode(f.read()).decode("utf-8")

        commit_message = f"è‡ªå‹•æ›´æ–°: æ¤œæŸ»å·¥æ•°ãƒ‡ãƒ¼ã‚¿ ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')})"
        data = {
            "message": commit_message,
            "content": encoded_content,
            "branch": branch
        }
        if sha:
            data["sha"] = sha

        put_resp = requests.put(api_url, headers=headers, json=data)

        if put_resp.status_code in [200, 201]:
            return JSONResponse(content={
                "status": "success",
                "message": f"{len(new_df)} ä»¶ã®æ–°è¦ãƒ»æ›´æ–°ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä¿å­˜ã—ã€GitHub ã«åæ˜ ã—ã¾ã—ãŸ"
            })
        else:
            return JSONResponse(content={
                "status": "partial_success",
                "message": f"ä¿å­˜æˆåŠŸãƒ»GitHubåæ˜ å¤±æ•—: {put_resp.json()}"
            }, status_code=500)

    except Exception as e:
        return JSONResponse(content={
            "status": "error",
            "message": f"ä¿å­˜æˆåŠŸãƒ»GitHubé€£æºå¤±æ•—: {str(e)}"
        }, status_code=500)

@app.post("/api/receive_kousu_data")
async def receive_kousu_data(records: UploadFile = File(...)):
    contents = await records.read()

    # CSVèª­ã¿è¾¼ã¿
    try:
        new_df = pd.read_csv(io.BytesIO(contents), encoding="utf-8-sig")
    except UnicodeDecodeError:
        new_df = pd.read_csv(io.BytesIO(contents), encoding="cp932")

    # âœ… ã‚«ãƒ©ãƒ åã®å‰å¾Œã‚¹ãƒšãƒ¼ã‚¹å‰Šé™¤ï¼‹å…¨è§’ã‚«ãƒƒã‚³â†’åŠè§’ã¸æ­£è¦åŒ–
    new_df.columns = [col.strip().replace("ï¼ˆ", "(").replace("ï¼‰", ")").replace('"', "").replace("'", "") for col in new_df.columns]
    print("ğŸ“‹ ä¿®æ­£å¾Œã‚«ãƒ©ãƒ :", new_df.columns.tolist())

    # âœ… ä½œæ¥­æ™‚é–“åˆ—ã®å‡¦ç†ï¼ˆã€Œä½œæ¥­æ™‚é–“ã€ã‚„ã€Œä½œæ¥­æ™‚é–“(m)ã€ã«å¯¾å¿œï¼‰
    time_col = next((col for col in new_df.columns if "ä½œæ¥­æ™‚é–“" in col), None)
    if time_col:
        new_df["ä½œæ¥­æ™‚é–“"] = pd.to_numeric(new_df[time_col], errors="coerce")
    else:
        new_df["ä½œæ¥­æ™‚é–“"] = 0.0

    # âœ… æœŸå¾…ã‚«ãƒ©ãƒ ã‚’æŠ½å‡ºãƒ»æ•´å½¢
    expected_cols = ["ä½œæ¥­ID", "ä½œæ¥­æ—¥", "ä½œæ¥­å®Ÿæ–½è€…", "ä½œæ¥­ç¨®åˆ¥", "ä½œæ¥­æ™‚é–“"]
    new_df = new_df[[col for col in new_df.columns if col in expected_cols]]
    new_df = new_df.reindex(columns=expected_cols)

    # âœ… ä¿å­˜å…ˆã®CSVã‚’èª­ã¿è¾¼ã¿ï¼ˆãªã‘ã‚Œã°ç©ºã®DataFrameã‚’ç”¨æ„ï¼‰
    os.makedirs("data", exist_ok=True)
    save_path = os.path.join("data", "å·¥æ•°ãƒ‡ãƒ¼ã‚¿.csv")
    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã€ã‚µã‚¤ã‚ºãŒ0ã§ãªã„ã“ã¨ã‚’ç¢ºèªã—ã¦ã‹ã‚‰èª­ã¿è¾¼ã‚€
    if os.path.exists(save_path) and os.path.getsize(save_path) > 0:
        try:
            existing_df = pd.read_csv(save_path, encoding="utf-8-sig")
        except UnicodeDecodeError:
            try:
                existing_df = pd.read_csv(save_path, encoding="cp932")
            except Exception:
                existing_df = pd.DataFrame(columns=expected_cols)
    else:
        existing_df = pd.DataFrame(columns=expected_cols)


    # âœ… ç¢ºå®Ÿã«å¿…è¦åˆ—ã ã‘ã«ã—ã€indexã‚’ä½œæ¥­IDã«è¨­å®š
    existing_df.columns = [col.strip().replace("ï¼ˆ", "(").replace("ï¼‰", ")") for col in existing_df.columns]
    existing_df = existing_df[[col for col in existing_df.columns if col in expected_cols]]
    existing_df = existing_df.reindex(columns=expected_cols)

    try:
        existing_df.set_index("ä½œæ¥­ID", inplace=True)
        new_df.set_index("ä½œæ¥­ID", inplace=True)
    except KeyError:
        return JSONResponse(
            status_code=400,
            content={"status": "error", "message": "ä½œæ¥­IDåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚CSVåˆ—åã‚’ã”ç¢ºèªãã ã•ã„ã€‚"}
        )

    # âœ… ä¸Šæ›¸ãï¼‹è¿½åŠ å‡¦ç†
    updated_df = existing_df.combine_first(new_df)
    updated_df.update(new_df)

    # âœ… ä¿å­˜ãƒ»GitHubã¸Push
    updated_df.reset_index(inplace=True)
    updated_df.to_csv(save_path, index=False, encoding="utf-8-sig")

    # âœ… GitHubé€£æº
    try:
        GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
        repo_owner = "otameshi-web"
        repo_name = "kousu-app"
        branch = "master"
        file_path = "data/å·¥æ•°ãƒ‡ãƒ¼ã‚¿.csv"
        api_url = f"https://api.github.com/repos/{repo_owner}/{repo_name}/contents/{file_path}"

        headers = {
            "Authorization": f"Bearer {GITHUB_TOKEN}",
            "Accept": "application/vnd.github+json"
        }

        get_resp = requests.get(api_url, headers=headers, params={"ref": branch})
        sha = get_resp.json().get("sha", None)

        with open(save_path, "rb") as f:
            encoded_content = base64.b64encode(f.read()).decode("utf-8")

        commit_message = f"è‡ªå‹•æ›´æ–°: å·¥æ•°ãƒ‡ãƒ¼ã‚¿ ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')})"
        data = {
            "message": commit_message,
            "content": encoded_content,
            "branch": branch
        }
        if sha:
            data["sha"] = sha

        put_resp = requests.put(api_url, headers=headers, json=data)

        if put_resp.status_code in [200, 201]:
            return JSONResponse(content={
                "status": "success",
                "message": f"{len(new_df)} ä»¶ã®æ–°è¦ãƒ»æ›´æ–°ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä¿å­˜ã—ã€GitHub ã«åæ˜ ã—ã¾ã—ãŸ"
            })
        else:
            return JSONResponse(content={
                "status": "partial_success",
                "message": f"ä¿å­˜æˆåŠŸãƒ»GitHubåæ˜ å¤±æ•—: {put_resp.json()}"
            }, status_code=500)

    except Exception as e:
        return JSONResponse(content={
            "status": "error",
            "message": f"ä¿å­˜æˆåŠŸãƒ»GitHubé€£æºå¤±æ•—: {str(e)}"
        }, status_code=500)


# ==========================
#    renderã‚’GitHubã‹ã‚‰èµ·å‹•
# ==========================
@app.get("/healthcheck")
def healthcheck():
    return JSONResponse(content={"status": "ok", "message": "healthcheck successful"})

